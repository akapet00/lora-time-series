%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt, a4paper]{article} % 10pt font size (11 and 12 also possible), A4 paper (letterpaper for US letter) and two column layout (remove for one column)

\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{Estimation of the Probability of Successful Transmission of Uplink Messages in LoRaWAN} % The article title

\author{
	\authorstyle{Ante Lojić Kapetanović}
	\newline\newline % Space before institutions
	\institution{University of Split, Split, Croatia}
}

% Example of a one line author/institution relationship
%\author{\newauthor{John Marston} \newinstitution{Universidad Nacional Autónoma de México, Mexico City, Mexico}}

\date{\today} % Add a date here if you would like one to appear underneath the title block, use \today for the current date, leave empty for no date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{This report is a short overview of applying forecasting methods on data from real LoRaWAN deployment in Svebølle, Denmark. The main idea is to examine whether there is a possibility of predicting successful transmission of LoRa messages for the given future time interval. Proposed model for prediction is Long Short-Term Memory (LSTM), an artificial recurrent neural network (RNN) architecture used in the field of deep learning.}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}
TBA

\section{Network Topology}

LoRaWAN - Long Range Wide Area Network - low power, wide area media access protocol designed to wirelessly connect 'things' to the internet. LoRaWAN is useful in IoT deployment, specially in smart environments where there are thousands of end-devices (i.e. sensors) that require minimal bandwidth and low power consumption.

The network architecutre is deployed in a star-of-stars topology in which gateways relay messages between end-devices and a central network server \cite{Silva_LoRaWAN}.
Toplogy overview is shown in Fig. \ref{lorawan}.
\begin{figure}
	\includegraphics[width=\linewidth]{images/LoRaWAN-Network-Layout.png} % Figure image
	\caption{LoRaWAN Network Layout} % Figure caption
	\label{lorawan} % Label for referencing with \ref{bear}
\end{figure}


End-device is transmitting data to base station using LoRa technology, which means that data is modulated using CSS technique and transmitted using 868MHz RF, also it is worth mentioning that data is encrypted using AES128 in Counter mode (CTR). If the packet’s FPort is set to 0 then the NwkSKey is used, otherwise the AppSKey is used. An important feature of all messages in LoRa is that the counters for sent (FCntUp) and received (FCntDown) messages are maintained by the Node and Network Server, and that these counters never repeat \cite{Miller_LoRaSecurity}.
Base stations relay messages from end-nodes to network server using a back-haul wireless network with higher frequency (often 2.4GHz) or using wired connection (Ethernet, optics). 
Network server decodes the packets received from base station and performs security checks and adaptive data rate so it can generate the packets that should be sent back to end-devices \cite{Silva_LoRaWAN}.

Observed LoRaWAN network model deployed in Svebølle is consisted of a single base station which communicates with few hundreds of different end-devices; network topology is quite simple and shown on Fig. \ref{svebolle}.
\begin{figure}
	\centering
	\includegraphics[scale=.7]{images/Svebolle-Topology.png} % Figure image
	\caption{Overview of Svebølle LoRaWAN network} % Figure caption
	\label{svebolle} 
\end{figure}

Data was collected during the period of 4 months and 14 days, during which approximately 689 thousands measurements were generated and captured. 
There is only one base station and, judgding by the exploration of the data set, 251 end-devices in the network.
Measurements were carried out on the base station side, which forwards data to network server, shown in Fig. \ref{svebolle}. There are more than few relevant measured features:
\begin{itemize}
	\item Time - basic datetime format to microsecond precision
    \item DevAddr - device address written as HEX string
    \item Freq - carrier signal radio frequency
    \item Chan - channel number
    \item BW - bandwidth
    \item SPF - spreading factor
    \item RSSI - received signal strength indication
    \item SNR - Signal-to-Noise ratio
    \item CR - code rate 
    \item DR - data rate
    \item crcStatus - cyclic redundancy check
    \item mType - type of MAC message
    \item macPayload - part of transmitted data that is the actual intended message
    \item id - packet identification
\end{itemize}

All of these indicators are described in details in \cite{Aloys_LoRa}.

Considering the data, simple communication model of this LoRaWAN network could be deduced and is shown in Fig. \ref{communication}
\begin{figure}
	\centering
	\includegraphics[scale=.5]{images/Svebolle-ed-bs-model.png} % Figure image
	\caption{Communication between end-device and base station in LoRaWAN network in Svebølle} % Figure caption
	\label{communication} 
\end{figure}

\section{Introducing ANN}

Measurements in data set were not structured as time-series data, only times when transmission of LoRa message was successful in uplink direction are captured and stored. 
Time series data is a series of data points indexed in time order and taken at successive equally spaced points in time. A time series is generally described through parameters such as trend, seasonality, irregularity and cyclicallity \cite{Adhikari_timeseries}.

In order to create time series data, the only important label is wheter the end-device has successfully transmitted the message to the gateway or not. 
For each time point, where every time point is observation in seconds, there is activity flag assigned. If the device was active for observed time point, activity flag is 1 otherwise activity flag is 0.
Since the measurements were carried out through almost 5 month period, newly created data set was huge and as such perfect candidate for applying deep learning methods to perform forecasting of future time points.

Artificial Neural Networks, or ANN for short, are modeling the intelligence of human brain: recognizing regularities and patterns of the given data, learning from past experience and providing inference on the output \cite{Adhikari_timeseries}.
Why are ANNs such good predictors for time series type of data? There are couple of reasons:
\begin{description}
	\item[Data-driven and self-adaptive] There is no need to make \emph{a priori} assumption about statistaical distribution of the data before feeding ANN with the data \cite{Adhikari_timeseries}.
	\item [Non-linearity] ANNs are perfect for modeling data with non-obvious patterns \cite{Zhang_ann}.
	\item [Universal functional approximators] Any contionous function can be approximated to any accuracy \cite{Hornik_ann}.
\end{description}

Even though vanilla ANNs are very powerful tool to create predictions based on historical events, they lack persistence. 
Recurrent Neural Networks (RNNs) are trying to solve this problem implementing loops and allowing information to persist, overview of RNN is shown in Fig. \ref{rnn}. RNN looks at the input $ x_{t} $ and outputs a value $ h_{t} $, a loop allows information to be passed from one step of the network to the next. 
\begin{figure}
	\centering
	\includegraphics[scale=.5]{images/RNN.png} % Figure image
	\caption{RNN simple overview - feedback provides that the ouput (or a hidden layer) of the network, together with the new input at the current timestamp, is taken as input}
	\label{rnn} 
\end{figure}
Unpacked RNNs have the form of a chain of repeating nodes defined by very simple structure, Fig. \ref{rnn-unpacked} \cite{Olah_lstm}.
\begin{figure}
	\centering
	\includegraphics[scale=.5]{images/RNN-unpacked.png} % Figure image
	\caption{RNN are formed in a chain-like nature what makes them applicable for predicting future events of sequence data}
	\label{rnn-unpacked} 
\end{figure}

RNNs are using gradient descent (first-order iterative optimization algorithm, usually used for finding the minimum of a function) as a way to minimize the error caused by changing each weight in proportion the derivative of the error with respect to that weight. Problem with this approach is error gradient vanishing which is causing the RNN to become unable to learn to connect the information if the relevant data is devided by large gap. 
Vanishng gradients problem happens when the leading eigenvalue of the weight matrix becomes too small. This sitation can lead to gradient signal becoming so small that learning becomes extremely slow and, eventually, stops working. 
The problem of long-term dependencies is solved by applying Long Short-Term Memory network, LSTM for short, which are introduced by the Sepp Hochreiter and Jurgen Schmidhuber in 1997. in the \cite{Hochreiter_LSTM}. They proposed a model that is capable of remembering relevant information for long period of time and avoiding the long-term dependency issues. 

\section{Proposed LSTM model}

Just like the RNN, LSTM architecture is also formed as a chain structure. Difference is that every cell has four in-layers each performing specific operations on input data and interacting with others in a special way, general overview of LSTM network is shown on Fig. \ref{lstm}.
\begin{figure}
	\centering
	\includegraphics[scale=.4]{images/LSTM.png} % Figure image
	\caption{LSTM network preview}
	\label{lstm} 
\end{figure}
Green boxes represent cells (or neurons) of the network. Since LSTM is recurrent network, both new input $ x_{t} $ and the output of previous cell $ h_{t-1} $ are fed to the current cell.

An LSTM cell is consisted of five components which allow it to model both long-term and short-term data: cell state at time $ t $ ($ C_{t} $); hidden state ($ \tilde{C}_{t}) $ ; input gate ($ i_{t} $); forget gate ($ f_{t} $) and output gate ($ o_{t} $).
Cell state, presented with the upper horizontal line in Fig. \ref{lstm}, runs through the whole cell and is affected by some minor linear changes.

Gates are the mechanism that allows information to be passed in selective fashion from the LSTM to the cell state. They consist of a sigmoid layer, which outputs value between 0 and 1 describing value of each component and a multiplication operation.
There are three types of gates in LSTM network:
\begin{itemize}
	\item {Forget gate} - determines which information to remove from the cell state using sigmoid function that outputs value between 0 (completely reject) and 1 (completely accept)
		$$ f_{t} = \sigma (W_{f} \cdot [h_{t-1}, x_{t}] + b_{f}) $$
		where
		\begin{itemize}
			\item[] $ x_{t} $ is current input vector,
			\item[] $ h_{t-1} $ is previous cell's output value,
			\item[] $ W_{f} $ is associated weight,
			\item[] $ b_{f} $ is added bias.
		\end{itemize}
	\item {The second gate} - decides what new information should be written to the cell state. 
	
	Firstly, the sigmoid function decides which value to update:
	$$ i_{t} = \sigma (W_{i} \cdot [h_{t-1}, x_{t}] + b_{i}) $$ 
	where
	\begin{itemize}
		\item[] $ x_{t} $ is current input vector,
		\item[] $ h_{t-1} $ is previous cell's output value,
		\item[] $ W_{i} $ is associated weight,
		\item[] $ b_{i} $ is added bias.
	\end{itemize}

	After that, the \emph{tanh} layer creates a vector of candidates to be added to the cell state. 
	$$ \tilde{C}_{t} = tanh (W_{C} \cdot [h_{t-1}, x_{t}] + b_{C}) $$
	where
	\begin{itemize}
		\item[] $ x_{t} $ is current input vector,
		\item[] $ h_{t-1} $ is previous cell's output value,
		\item[] $ W_{C} $ is associated weight,
		\item[] $ b_{C} $ is added bias.
	\end{itemize}
	\item {The output gate} - decides what is going to be output of the concrete cell. 
	$$ o_{t} = \theta (W_{o} [h_{t-1}, x_{t}] + b_{o}) $$ 
	where
	\begin{itemize}
		\item[] $ x_{t} $ is current input vector,
		\item[] $ h_{t-1} $ is previous cell's output value,
		\item[] $ W_{o} $ is associated weight,
		\item[] $ b_{o} $ is added bias.
	\end{itemize}
	
	The output is based on the filtered version of the cell state.
	$$ h_{t} = o_{t} * tanh(C_{t})$$
	where 
	$ C_{t} $ is the new cell state calclulated as:
	$ C_{t} = f_{t} * C_{t-1} + i_{t} * \tilde{C}_{t} $
\end{itemize}

In this case, concrete LSTM model for predicting time series data was developed using Keras, open source neural network library written in Python which is capable of running on top of TensorFlow. 
Model consists of 4 LSTM layers where first two layers are consisted of 128 neurons, third layer has 10 neurons and fourth layer, output layer, is consisted of only 1 neuron.
Additional dropout and normalization layers are added after each LSTM layer except the output layer.  
The main idea was that the LSTM model successfully predicts when will transmission of the observed device happend. 
After the model had been built, network was fed using data consisting out of sequential time points with joined 0 or 1 depending on the success of transmission of LoRa message for that moment.
Before the actual load of the data, some preprocessing is required.
The way LSTM layers work is by taking in an array of 3 dimensions (N, W, F) where N is the number of training sequences, W is the sequnce length and F is the number of features of each sequence. 
Sequnce length can be treated as hyperparameter whose value will affect (posibbly improve) overall quality of the model. Sequnce number is, in this case, arbitrary sized window which gives network the ability to detailly observe the data and learn how to build up a pattern of sequences based on the received sequental data.  
The sequnces are constructed as sliding windows shifted by 1 after every iteration. 
Number of feature is in this case one since its purpose is just define activity of the specific device. 

The model is built by having a potential to predict single time point in the future or to predict a full sequnce (error prone).

\section{Results}

After only one training epoch results were starting to look promising. 
Training was performed on 80 percent of data, where 20 percent of the data was left for testing and evaluation.
On the Fig. \ref{lstm-out}, blue lines are representing actual device activity while the red triangles are predicting, based on historical events, when the actual events should happend. 
On the x-axis each point is representing one second in the future where 0 second is presenting current moment, on the y-axis there can only be 1 or 0, for device will be active in the specific second or it won't be active in the specific second in the future.
\begin{figure}
	\centering
	\includegraphics[scale=.6]{images/lstm-out.png} % Figure image
	\caption{Processed LSTM output is represented with red triangles which estimate moments in which observed device is active (y=1) or not active (y=0) while blue lines represent actual device activation }
	\label{lstm-out} 
\end{figure}

The core idea of this project is to assess the probability that device will be active for given time period in the future.
$$ \mathbb{P}[N_{t} = k | X] =  \frac{1}{s-t}  \sum_{i=0}^{s-t}\mathbbm{1}[\sum_{j=0}^{t}X[i+j]=k] $$
Estimation of successful transmission of the message for specific LoRa device for the observed subsample is shown in the equation above.
\begin{itemize}
	\item First, for the $ t $ number of seconds we want to define estimate of probability, we count how many times device was active.
	\item The same process is applied on the next $ t $ seconds in subsample where next $ t $ seconds represent time window moved by 1 second in the direction of x-axis.
	\item When the count for the whole sample is finished, we divide the number of device activations and number of seconds of sample $ s $. 
\end{itemize} 
As window size (number of seconds for which we want to estimate probability of activation) rises so does the probability of activation. Finally, for the window size equals to sample size, probability should be 1 in most cases. 
Probability distribution is shown on Fig. \ref{est-prob}.
\begin{figure}
	\centering
	\includegraphics[scale=.6]{images/est-prob.png} % Figure image
	\caption{Estimation of probability of device activation on the test data}
	\label{est-prob} 
\end{figure}


\section{Conclusion}
TBA

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography

%----------------------------------------------------------------------------------------

\end{document}
